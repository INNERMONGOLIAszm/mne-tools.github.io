{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Visualising statistical significance thresholds on EEG data\n\n\nMNE-Python provides a range of tools for statistical hypothesis testing\nand the visualisation of the results. Here, we show a few options for\nexploratory and confirmatory tests - e.g., targeted t-tests, cluster-based\npermutation approaches (here with Threshold-Free Cluster Enhancement);\nand how to visualise the results.\n\nThe underlying data comes from [1]_; we contrast long vs. short words.\nTFCE is described in [2]_.\n\nReferences\n----------\n.. [1] Dufau, S., Grainger, J., Midgley, KJ., Holcomb, PJ. A thousand\n   words are worth a picture: Snapshots of printed-word processing in an\n   event-related potential megastudy. Psychological Science, 2015\n.. [2] Smith and Nichols 2009, \"Threshold-free cluster enhancement:\n   addressing problems of smoothing, threshold dependence, and\n   localisation in cluster inference\", NeuroImage 44 (2009) 83-98.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom scipy.stats import ttest_ind\nimport numpy as np\n\nimport mne\nfrom mne.channels import find_layout, find_ch_connectivity\nfrom mne.stats import spatio_temporal_cluster_test\n\nnp.random.seed(0)\n\n# Load the data\npath = mne.datasets.kiloword.data_path() + '/kword_metadata-epo.fif'\nepochs = mne.read_epochs(path)\nname = \"NumberOfLetters\"\n\n# Split up the data by the median length in letters via the attached metadata\nmedian_value = str(epochs.metadata[name].median())\nlong = epochs[name + \" > \" + median_value]\nshort = epochs[name + \" < \" + median_value]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a specific point in space and time we wish to test, it can be\nconvenient to convert the data into Pandas Dataframe format. In this case,\nthe :class:`mne.Epochs` object has a convenient\n:meth:`mne.Epochs.to_data_frame` method, which returns a dataframe.\nThis dataframe can then be queried for specific time windows and sensors.\nThe extracted data can be submitted to standard statistical tests. Here,\nwe conduct t-tests on the difference between long and short words.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_windows = ((200, 250), (350, 450))\nelecs = [\"Fz\", \"Cz\", \"Pz\"]\n\n# display the EEG data in Pandas format (first 5 rows)\nprint(epochs.to_data_frame()[elecs].head())\n\nreport = \"{elec}, time: {tmin}-{tmax} msec; t({df})={t_val:.3f}, p={p:.3f}\"\nprint(\"\\nTargeted statistical test results:\")\nfor (tmin, tmax) in time_windows:\n    for elec in elecs:\n        # extract data\n        time_win = \"{} < time < {}\".format(tmin, tmax)\n        A = long.to_data_frame().query(time_win)[elec].groupby(\"condition\")\n        B = short.to_data_frame().query(time_win)[elec].groupby(\"condition\")\n\n        # conduct t test\n        t, p = ttest_ind(A.mean(), B.mean())\n\n        # display results\n        format_dict = dict(elec=elec, tmin=tmin, tmax=tmax,\n                           df=len(epochs.events) - 2, t_val=t, p=p)\n        print(report.format(**format_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Absent specific hypotheses, we can also conduct an exploratory\nmass-univariate analysis at all sensors and time points. This requires\ncorrecting for multiple tests.\nMNE offers various methods for this; amongst them, cluster-based permutation\nmethods allow deriving power from the spatio-temoral correlation structure\nof the data. Here, we use TFCE.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate statistical thresholds\ncon = find_ch_connectivity(epochs.info, \"eeg\")\n\n# Extract data: transpose because the cluster test requires channels to be last\n# In this case, inference is done over items. In the same manner, we could\n# also conduct the test over, e.g., subjects.\nX = [long.get_data().transpose(0, 2, 1),\n     short.get_data().transpose(0, 2, 1)]\ntfce = dict(start=.2, step=.2)\n\nt_obs, clusters, cluster_pv, h0 = spatio_temporal_cluster_test(\n    X, tfce, n_permutations=100)\nsignificant_points = cluster_pv.reshape(t_obs.shape).T < .05\nprint(str(significant_points.sum()) + \" points selected by TFCE ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results of these mass univariate analyses can be visualised by plotting\n:class:`mne.Evoked` objects as images (via :class:`mne.Evoked.plot_image`)\nand masking points for significance.\nHere, we group channels by Regions of Interest to facilitate localising\neffects on the head.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We need an evoked object to plot the image to be masked\nevoked = mne.combine_evoked([long.average(), -short.average()],\n                            weights='equal')  # calculate difference wave\ntime_unit = dict(time_unit=\"s\")\nevoked.plot_joint(title=\"Long vs. short words\", ts_args=time_unit,\n                  topomap_args=time_unit)  # show difference wave\n\n# Create ROIs by checking channel labels\npos = find_layout(epochs.info).pos\nrois = dict()\nfor pick, channel in enumerate(epochs.ch_names):\n    last_char = channel[-1]  # for 10/20, last letter codes the hemisphere\n    roi = (\"Midline\" if last_char in \"z12\" else\n           (\"Left\" if int(last_char) % 2 else \"Right\"))\n    rois[roi] = rois.get(roi, list()) + [pick]\n\n# sort channels from front to center\n# (y-coordinate of the position info in the layout)\nrois = {roi: np.array(picks)[pos[picks, 1].argsort()]\n        for roi, picks in rois.items()}\n\n# Visualize the results\nfig, axes = plt.subplots(nrows=3, figsize=(8, 8))\nvmax = np.abs(evoked.data).max() * 1e6\n\n# Iterate over ROIs and axes\naxes = axes.ravel().tolist()\nfor roi_name, ax in zip(sorted(rois.keys()), axes):\n    picks = rois[roi_name]\n    evoked.plot_image(picks=picks, axes=ax, colorbar=False, show=False,\n                      clim=dict(eeg=(-vmax, vmax)), mask=significant_points,\n                      **time_unit)\n    evoked.nave = None\n    ax.set_yticks((np.arange(len(picks))) + .5)\n    ax.set_yticklabels([evoked.ch_names[idx] for idx in picks])\n    if not ax.is_last_row():  # remove xticklabels for all but bottom axis\n        ax.set(xlabel='', xticklabels=[])\n    ax.set(ylabel='', title=roi_name)\n\nfig.colorbar(ax.images[-1], ax=axes, fraction=.1, aspect=20,\n             pad=.05, shrink=2 / 3, label=\"uV\", orientation=\"vertical\")\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}