.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_tutorials_plot_stats_cluster_erp.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_plot_stats_cluster_erp.py:


===========================================================================
Visualising statistical significance thresholds on EEG data
===========================================================================

MNE-Python provides a range of tools for statistical hypothesis testing
and the visualisation of the results. Here, we show a few options for
exploratory and confirmatory tests - e.g., targeted t-tests, cluster-based
permutation approaches (here with Threshold-Free Cluster Enhancement);
and how to visualise the results.

The underlying data comes from [1]_; we contrast long vs. short words.
TFCE is described in [2]_.

References
----------
.. [1] Dufau, S., Grainger, J., Midgley, KJ., Holcomb, PJ. A thousand
   words are worth a picture: Snapshots of printed-word processing in an
   event-related potential megastudy. Psychological Science, 2015
.. [2] Smith and Nichols 2009, "Threshold-free cluster enhancement:
   addressing problems of smoothing, threshold dependence, and
   localisation in cluster inference", NeuroImage 44 (2009) 83-98.


.. code-block:: python


    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.stats import ttest_ind

    import mne
    from mne.channels import find_ch_connectivity, make_1020_channel_selections
    from mne.stats import spatio_temporal_cluster_test

    np.random.seed(0)

    # Load the data
    path = mne.datasets.kiloword.data_path() + '/kword_metadata-epo.fif'
    epochs = mne.read_epochs(path)
    name = "NumberOfLetters"

    # Split up the data by the median length in letters via the attached metadata
    median_value = str(epochs.metadata[name].median())
    long_words = epochs[name + " > " + median_value]
    short_words = epochs[name + " < " + median_value]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Reading /home/circleci/mne_data/MNE-kiloword-data/kword_metadata-epo.fif ...
    Isotrak not found
        Found the data of interest:
            t =    -100.00 ...     920.00 ms
            0 CTF compensation matrices available
    960 matching events found
    No baseline correction applied
    960 matching events found
    No baseline correction applied
    Adding metadata with 8 columns
    0 projection items activated


If we have a specific point in space and time we wish to test, it can be
convenient to convert the data into Pandas Dataframe format. In this case,
the :class:`mne.Epochs` object has a convenient
:meth:`mne.Epochs.to_data_frame` method, which returns a dataframe.
This dataframe can then be queried for specific time windows and sensors.
The extracted data can be submitted to standard statistical tests. Here,
we conduct t-tests on the difference between long and short words.


.. code-block:: python


    time_windows = ((.2, .25), (.35, .45))
    elecs = ["Fz", "Cz", "Pz"]

    # display the EEG data in Pandas format (first 5 rows)
    print(epochs.to_data_frame()[elecs].head())

    report = "{elec}, time: {tmin}-{tmax} s; t({df})={t_val:.3f}, p={p:.3f}"
    print("\nTargeted statistical test results:")
    for (tmin, tmax) in time_windows:
        long_df = long_words.copy().crop(tmin, tmax).to_data_frame()
        short_df = short_words.copy().crop(tmin, tmax).to_data_frame()
        for elec in elecs:
            # extract data
            A = long_df[elec].groupby("condition").mean()
            B = short_df[elec].groupby("condition").mean()

            # conduct t test
            t, p = ttest_ind(A, B)

            # display results
            format_dict = dict(elec=elec, tmin=tmin, tmax=tmax,
                               df=len(epochs.events) - 2, t_val=t, p=p)
            print(report.format(**format_dict))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Converting time column to int64...
    signal                      Fz  ...        Pz
    condition epoch time            ...          
    film      0     -100  0.421970  ...  0.398182
                    -96   0.453939  ...  0.222424
                    -92   0.465606  ...  0.018485
                    -88   0.468182  ... -0.173485
                    -84   0.483485  ... -0.312121

    [5 rows x 3 columns]

    Targeted statistical test results:
    Converting time column to int64...
    Converting time column to int64...
    Fz, time: 0.2-0.25 s; t(958)=-0.572, p=0.568
    Cz, time: 0.2-0.25 s; t(958)=-2.836, p=0.005
    Pz, time: 0.2-0.25 s; t(958)=-3.938, p=0.000
    Converting time column to int64...
    Converting time column to int64...
    Fz, time: 0.35-0.45 s; t(958)=5.192, p=0.000
    Cz, time: 0.35-0.45 s; t(958)=5.555, p=0.000
    Pz, time: 0.35-0.45 s; t(958)=6.353, p=0.000


Absent specific hypotheses, we can also conduct an exploratory
mass-univariate analysis at all sensors and time points. This requires
correcting for multiple tests.
MNE offers various methods for this; amongst them, cluster-based permutation
methods allow deriving power from the spatio-temoral correlation structure
of the data. Here, we use TFCE.


.. code-block:: python


    # Calculate statistical thresholds
    con = find_ch_connectivity(epochs.info, "eeg")

    # Extract data: transpose because the cluster test requires channels to be last
    # In this case, inference is done over items. In the same manner, we could
    # also conduct the test over, e.g., subjects.
    X = [long_words.get_data().transpose(0, 2, 1),
         short_words.get_data().transpose(0, 2, 1)]
    tfce = dict(start=.2, step=.2)

    t_obs, clusters, cluster_pv, h0 = spatio_temporal_cluster_test(
        X, tfce, n_permutations=100)  # a more standard number would be 1000+
    significant_points = cluster_pv.reshape(t_obs.shape).T < .05
    print(str(significant_points.sum()) + " points selected by TFCE ...")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Could not find a connectivity matrix for the data. Computing connectivity based on Delaunay triangulations.
    -- number of connected vertices : 29
    stat_fun(H1): min=0.000000 max=81.298503
    Running initial clustering
    Using 406 thresholds from 0.20 to 81.20 for TFCE computation (h_power=2.00, e_power=0.50)
    Found 7424 clusters
    Permuting 99 times...

    Computing cluster p-values
    Done.
    1461 points selected by TFCE ...


The results of these mass univariate analyses can be visualised by plotting
:class:`mne.Evoked` objects as images (via :class:`mne.Evoked.plot_image`)
and masking points for significance.
Here, we group channels by Regions of Interest to facilitate localising
effects on the head.


.. code-block:: python


    # We need an evoked object to plot the image to be masked
    evoked = mne.combine_evoked([long_words.average(), -short_words.average()],
                                weights='equal')  # calculate difference wave
    time_unit = dict(time_unit="s")
    evoked.plot_joint(title="Long vs. short words", ts_args=time_unit,
                      topomap_args=time_unit)  # show difference wave

    # Create ROIs by checking channel labels
    selections = make_1020_channel_selections(evoked.info, midline="12z")

    # Visualize the results
    fig, axes = plt.subplots(nrows=3, figsize=(8, 8))
    axes = {sel: ax for sel, ax in zip(selections, axes.ravel())}
    evoked.plot_image(axes=axes, group_by=selections, colorbar=False, show=False,
                      mask=significant_points, show_names="all", titles=None,
                      **time_unit)
    plt.colorbar(axes["Left"].images[-1], ax=list(axes.values()), shrink=.3,
                 label="uV")

    plt.show()



.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /auto_tutorials/images/sphx_glr_plot_stats_cluster_erp_001.png
            :class: sphx-glr-multi-img

    *

      .. image:: /auto_tutorials/images/sphx_glr_plot_stats_cluster_erp_002.png
            :class: sphx-glr-multi-img





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  23.956 seconds)

**Estimated memory usage:**  10 MB


.. _sphx_glr_download_auto_tutorials_plot_stats_cluster_erp.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_stats_cluster_erp.py <plot_stats_cluster_erp.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_stats_cluster_erp.ipynb <plot_stats_cluster_erp.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
