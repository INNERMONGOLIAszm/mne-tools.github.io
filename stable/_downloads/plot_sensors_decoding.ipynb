{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n=================================\nDecoding sensor space data (MVPA)\n=================================\n\nDecoding, a.k.a MVPA or supervised machine learning applied to MEG\ndata in sensor space. Here the classifier is applied to every time\npoint.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\nimport mne\nfrom mne.datasets import sample\nfrom mne.decoding import (SlidingEstimator, GeneralizingEstimator,\n                          cross_val_multiscore, LinearModel, get_coef)\n\ndata_path = sample.data_path()\n\nplt.close('all')\n\n# sphinx_gallery_thumbnail_number = 4"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Set parameters\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "raw_fname = data_path + '/MEG/sample/sample_audvis_raw.fif'\ntmin, tmax = -0.200, 0.500\nevent_id = dict(audio_left=1, visual_left=3)\n\n# Setup for reading the raw data\nraw = mne.io.read_raw_fif(raw_fname, preload=True)\n\n# The subsequent decoding analyses only capture evoked responses, so we can\n# low-pass the MEG data. Usually a value more like 40 Hz would be used,\n# but here low-pass at 20 so we can more heavily decimate, and allow\n# the examlpe to run faster.\nraw.filter(None, 20., fir_design='firwin')\nevents = mne.find_events(raw, 'STI 014')\n\n# Set up pick list: EEG + MEG - bad channels (modify to your needs)\nraw.info['bads'] += ['MEG 2443', 'EEG 053']  # bads + 2 more\npicks = mne.pick_types(raw.info, meg='grad', eeg=False, stim=True, eog=True,\n                       exclude='bads')\n\n# Read epochs\nepochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=True,\n                    picks=picks, baseline=(None, 0.), preload=True,\n                    reject=dict(grad=4000e-13, eog=150e-6), decim=10)\nepochs.pick_types(meg=True, exclude='bads')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Temporal decoding\n-----------------\n\nWe'll use a Logistic Regression for a binary classification as machine\nlearning model.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# We will train the classifier on all left visual vs auditory trials on MEG\n\nX = epochs.get_data()  # MEG signals: n_epochs, n_channels, n_times\ny = epochs.events[:, 2]  # target: Audio left or right\n\nclf = make_pipeline(StandardScaler(), LogisticRegression())\n\ntime_decod = SlidingEstimator(clf, n_jobs=1, scoring='roc_auc')\n\nscores = cross_val_multiscore(time_decod, X, y, cv=5, n_jobs=1)\n\n# Mean scores across cross-validation splits\nscores = np.mean(scores, axis=0)\n\n# Plot\nfig, ax = plt.subplots()\nax.plot(epochs.times, scores, label='score')\nax.axhline(.5, color='k', linestyle='--', label='chance')\nax.set_xlabel('Times')\nax.set_ylabel('AUC')  # Area Under the Curve\nax.legend()\nax.axvline(.0, color='k', linestyle='-')\nax.set_title('Sensor space decoding')\nplt.show()\n\n# You can retrieve the spatial filters and spatial patterns if you explicitly\n# use a LinearModel\nclf = make_pipeline(StandardScaler(), LinearModel(LogisticRegression()))\ntime_decod = SlidingEstimator(clf, n_jobs=1, scoring='roc_auc')\ntime_decod.fit(X, y)\n\ncoef = get_coef(time_decod, 'patterns_', inverse_transform=True)\nevoked = mne.EvokedArray(coef, epochs.info, tmin=epochs.times[0])\nevoked.plot_joint(times=np.arange(0., .500, .100), title='patterns')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Temporal Generalization\n-----------------------\n\nThis runs the analysis used in [1]_ and further detailed in [2]_\n\nThe idea is to fit the models on each time instant and see how it\ngeneralizes to any other time point.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# define the Temporal Generalization object\ntime_gen = GeneralizingEstimator(clf, n_jobs=1, scoring='roc_auc')\n\nscores = cross_val_multiscore(time_gen, X, y, cv=5, n_jobs=1)\n\n# Mean scores across cross-validation splits\nscores = np.mean(scores, axis=0)\n\n# Plot the diagonal (it's exactly the same as the time-by-time decoding above)\nfig, ax = plt.subplots()\nax.plot(epochs.times, np.diag(scores), label='score')\nax.axhline(.5, color='k', linestyle='--', label='chance')\nax.set_xlabel('Times')\nax.set_ylabel('AUC')\nax.legend()\nax.axvline(.0, color='k', linestyle='-')\nax.set_title('Decoding MEG sensors over time')\nplt.show()\n\n# Plot the full matrix\nfig, ax = plt.subplots(1, 1)\nim = ax.imshow(scores, interpolation='lanczos', origin='lower', cmap='RdBu_r',\n               extent=epochs.times[[0, -1, 0, -1]], vmin=0., vmax=1.)\nax.set_xlabel('Testing Time (s)')\nax.set_ylabel('Training Time (s)')\nax.set_title('Temporal Generalization')\nax.axvline(0, color='k')\nax.axhline(0, color='k')\nplt.colorbar(im, ax=ax)\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Exercise\n--------\n - Can you improve the performance using full epochs and a common spatial\n   pattern (CSP) used by most BCI systems?\n - Explore other datasets from MNE (e.g. Face dataset from SPM to predict\n   Face vs. Scrambled)\n\nHave a look at the example\n`sphx_glr_auto_examples_decoding_plot_decoding_csp_space.py`\n\nReferences\n==========\n\n.. [1] Jean-Remi King, Alexandre Gramfort, Aaron Schurger, Lionel Naccache\n       and Stanislas Dehaene, \"Two distinct dynamic modes subtend the\n       detection of unexpected sounds\", PLOS ONE, 2013,\n       http://www.ncbi.nlm.nih.gov/pubmed/24475052\n\n.. [2] King & Dehaene (2014) 'Characterizing the dynamics of mental\n       representations: the temporal generalization method', Trends In\n       Cognitive Sciences, 18(4), 203-210.\n       http://www.ncbi.nlm.nih.gov/pubmed/24593982\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.14", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}